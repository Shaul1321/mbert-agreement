{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0319 18:11:50.885721 140125443987264 file_utils.py:35] PyTorch version 1.3.1 available.\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/nlp/ravfogs/anaconda2/envs/py36/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "import csv\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from torch import nn\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Trainer\n",
    "from torch.utils.data import DataLoader\n",
    "from typing import List, Dict\n",
    "from torch.utils.data import Dataset\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load embedding P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.load(\"P.embeddings.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang2data = defaultdict(list)\n",
    "langs = [\"en\", \"fr\", \"ru\", \"de\", \"he\"]\n",
    "for lang in langs:\n",
    "    \n",
    "   dir_path = \"data/{}_replication_evalset\".format(lang)\n",
    "   for filename in os.listdir(dir_path):\n",
    "        \n",
    "        with open(dir_path+\"/\"+filename, \"r\", encoding = \"utf-8\") as f:\n",
    "            \n",
    "            lines = f.readlines()\n",
    "        \n",
    "        for line in lines:\n",
    "            \n",
    "            label, sent = line.strip().split(\"\\t\")\n",
    "            label = 1 if label == \"True\" else 0\n",
    "            sent += \".\"\n",
    "            filename_prefix = filename[:-4]\n",
    "            sent_dict = {\"text\": sent, \"label\": label, \"type\": filename_prefix}\n",
    "            lang2data[lang].append(sent_dict)\n",
    "            \n",
    "for lang in langs:\n",
    "    \n",
    "    random.shuffle(lang2data[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': 'the show the assistants like is new.', 'label': 1, 'type': 'obj_rel_no_comp_within_inanim'}\n"
     ]
    }
   ],
   "source": [
    "print(lang2data[\"en\"][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train-dev split for English, all other languages are evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7000\n",
    "data = lang2data[\"en\"][:n]\n",
    "l = int(0.8 * len(data))\n",
    "train, dev = data[:l], data[l:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'the painting the executive like is unpopular.',\n",
       " 'label': 0,\n",
       " 'type': 'obj_rel_no_comp_within_inanim'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pytorch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple torch dataset class\"\"\"\n",
    "\n",
    "    def __init__(self, data: List[Dict], device = \"cpu\"):\n",
    "\n",
    "        self.data = data\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            x,y = self.data[index][\"text\"], self.data[index][\"label\"]\n",
    "\n",
    "            return x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset = Dataset(dev[:50], \"cpu\"), Dataset(dev[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform INLP on embedding matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertModel(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, train_dataset: Dataset, dev_dataset: Dataset, batch_size, device: str, mode: str = \"eval\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.device = device\n",
    "        config = BertConfig.from_pretrained(\"bert-base-multilingual-uncased\", output_hidden_states=True, num_labels = 2)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "        self.model = BertForSequenceClassification.from_pretrained('bert-base-multilingual-uncased', config = config)\n",
    "        self.pad_token = self.tokenizer.convert_tokens_to_ids([self.tokenizer.pad_token])[0]\n",
    "        self.train_dataset = train_dataset\n",
    "        self.dev_dataset = dev_dataset\n",
    "        \n",
    "        if mode == \"eval\":\n",
    "            \n",
    "            self.model.eval()\n",
    "        else:\n",
    "            self.model.train()\n",
    "            \n",
    "        self.train_gen = torch.utils.data.DataLoader(self.train_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "        self.dev_gen = torch.utils.data.DataLoader(self.dev_dataset, batch_size=batch_size, drop_last=False, shuffle=True)\n",
    "        self.acc = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def tokenize_and_pad(self, texts: List[str]):\n",
    "        \n",
    "        indexed_texts = [self.tokenizer.encode(text, add_special_tokens=True) for text in texts] #\n",
    "        max_len = min(500, max(len(text) for text in indexed_texts))\n",
    "        indexed_texts = [text + [self.pad_token] * (max_len - len(text)) for text in indexed_texts]\n",
    "        return torch.LongTensor(indexed_texts).to(self.device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        outputs = self.model(x)\n",
    "        logits = outputs[0], outputs[1]\n",
    "        return logits\n",
    "    \n",
    "    def forward_with_loss_calculation(self, x, y):\n",
    "        \n",
    "        outputs = self.model(x, labels = y)\n",
    "        loss, logits = outputs[0], outputs[1]\n",
    "        return loss, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_nb):\n",
    "        \n",
    "        sents, y = batch\n",
    "        x = self.tokenize_and_pad(sents)        \n",
    "        loss, logits = self.forward_with_loss_calculation(x, y)\n",
    "\n",
    "        correct = logits.argmax(axis=1).int() == y.int()\n",
    "        acc = torch.sum(correct).float() / len(y)\n",
    "\n",
    "        return {'loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_step(self, batch, batch_nb):\n",
    "        sents, y = batch\n",
    "        x = self.tokenize_and_pad(sents)\n",
    "        loss, logits = self.forward_with_loss_calculation(x,y)\n",
    "\n",
    "        correct = logits.argmax(axis=1).int() == y.int()\n",
    "        acc = torch.sum(correct).float() / len(y)\n",
    "\n",
    "        return {'val_loss': loss, 'val_acc': acc}\n",
    "    \n",
    "    def validation_end(self, outputs):\n",
    "        avg_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
    "        avg_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
    "        print(\"Loss is {}\".format(avg_loss))\n",
    "        print(\"Accuracy is {}\".format(avg_acc))\n",
    "        self.acc = avg_acc\n",
    "        return {'avg_val_loss': avg_loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # return torch.optim.SGD(self.parameters(), lr=0.005, momentum=0.9)\n",
    "        return torch.optim.Adam(self.parameters(), weight_decay=1e-4)\n",
    "    \n",
    "    @pl.data_loader\n",
    "    def train_dataloader(self):\n",
    "        return self.train_gen\n",
    "\n",
    "    @pl.data_loader\n",
    "    def val_dataloader(self):\n",
    "        # OPTIONAL\n",
    "        # can also return a list of val dataloaders\n",
    "        return self.dev_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0319 18:15:48.059725 140125443987264 configuration_utils.py:185] loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-config.json from cache at /home/nlp/ravfogs/.cache/torch/transformers/33b56ce0f312e47e4d77a57791a4fc6233ae4a560dd2bdd186107058294e58ab.c7892120c5a9b21e515abc904e398dbabddf9510b122f659063cbf361fe16868\n",
      "I0319 18:15:48.062198 140125443987264 configuration_utils.py:199] Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"directionality\": \"bidi\",\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": true,\n",
      "  \"output_past\": true,\n",
      "  \"pooler_fc_size\": 768,\n",
      "  \"pooler_num_attention_heads\": 12,\n",
      "  \"pooler_num_fc_layers\": 3,\n",
      "  \"pooler_size_per_head\": 128,\n",
      "  \"pooler_type\": \"first_token_transform\",\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 105879\n",
      "}\n",
      "\n",
      "I0319 18:15:48.753724 140125443987264 tokenization_utils.py:398] loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt from cache at /home/nlp/ravfogs/.cache/torch/transformers/bb773818882b0524dc53a1b31a2cc95bc489f000e7e19773ba07846011a6c711.535306b226c42cebebbc0dabc83b92ab11260e9919e21e2ab0beb301f267b4c7\n",
      "I0319 18:15:49.635177 140125443987264 modeling_utils.py:406] loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-pytorch_model.bin from cache at /home/nlp/ravfogs/.cache/torch/transformers/cc4042a0d6f70eae595ea0e6d49521b17bd6251f973b3e37d303ce7945b90eed.54b4dad9cc3db9aa8448458b782d11ab07c80dedb951906fd2f684a00ecdb1ee\n",
      "I0319 18:15:54.825134 140125443987264 modeling_utils.py:480] Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "I0319 18:15:54.826486 140125443987264 modeling_utils.py:483] Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "bert = BertModel(dev_dataset, dev_dataset, batch_size = 4, device = \"cuda\", mode = \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(bert.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform INLP on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105879, 768) 105879\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "wv = bert.model.get_input_embeddings().weight.detach().cpu().numpy()\n",
    "#import requests\n",
    "vocab_url = \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased-vocab.txt\"\n",
    "\n",
    "page = urllib.request.urlopen(vocab_url)\n",
    "vocab = page.read().decode('utf8')\n",
    "vocab = vocab.split(\"\\n\")[:-1]\n",
    "print(wv.shape, len(vocab))\n",
    "w2i, i2w = {w:i for i,w in enumerate(vocab)}, {i:w for i,w in enumerate(vocab)}\n",
    "wv_cleaned = P.dot(wv.T).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c =  defaultdict(Counter)\n",
    "\n",
    "for lang, lang_data in lang2data.items():\n",
    "    for data_dict in lang_data:\n",
    "        sent = data_dict[\"text\"]\n",
    "        c[lang].update(sent.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "175"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c[\"en\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu available: True, used: True\n",
      "VISIBLE GPUS: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/26 [00:00<00:00, 46.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          Name                           Type  \\\n",
      "0                                        model  BertForSequenceClassification   \n",
      "1                                   model.bert                      BertModel   \n",
      "2                        model.bert.embeddings                 BertEmbeddings   \n",
      "3        model.bert.embeddings.word_embeddings                      Embedding   \n",
      "4    model.bert.embeddings.position_embeddings                      Embedding   \n",
      "..                                         ...                            ...   \n",
      "214                          model.bert.pooler                     BertPooler   \n",
      "215                    model.bert.pooler.dense                         Linear   \n",
      "216               model.bert.pooler.activation                           Tanh   \n",
      "217                              model.dropout                        Dropout   \n",
      "218                           model.classifier                         Linear   \n",
      "\n",
      "    Params  \n",
      "0    167 M  \n",
      "1    167 M  \n",
      "2     81 M  \n",
      "3     81 M  \n",
      "4    393 K  \n",
      "..     ...  \n",
      "214  590 K  \n",
      "215  590 K  \n",
      "216    0    \n",
      "217    0    \n",
      "218    1 K  \n",
      "\n",
      "[219 rows x 3 columns]\n",
      "Loss is 0.6653746962547302\n",
      "Accuracy is 0.6000000238418579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:02<00:00,  9.61it/s, batch_nb=12, epoch=0, gpu=0, loss=0.805, v_nb=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.6905643939971924\n",
      "Accuracy is 0.5384615659713745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:02<00:00, 11.67it/s, batch_nb=12, epoch=1, gpu=0, loss=0.793, v_nb=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.7584704756736755\n",
      "Accuracy is 0.5384615659713745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:02<00:00, 12.42it/s, batch_nb=12, epoch=2, gpu=0, loss=0.808, v_nb=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.847201406955719\n",
      "Accuracy is 0.5384615659713745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:02<00:00, 12.49it/s, batch_nb=12, epoch=3, gpu=0, loss=0.804, v_nb=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.7126203179359436\n",
      "Accuracy is 0.46153849363327026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26/26 [00:02<00:00, 12.72it/s, batch_nb=12, epoch=4, gpu=0, loss=0.779, v_nb=21]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss is 0.7199629545211792\n",
      "Accuracy is 0.5384615659713745\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(max_nb_epochs=10,min_nb_epochs=8, gpus = 1)\n",
    "trainer.fit(bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'the painting the executive like is unpopular.',\n",
       "  'label': 0,\n",
       "  'type': 'obj_rel_no_comp_within_inanim'},\n",
       " {'text': 'the show the assistants like is new.',\n",
       "  'label': 1,\n",
       "  'type': 'obj_rel_no_comp_within_inanim'},\n",
       " {'text': 'the teacher that likes the guard are young.',\n",
       "  'label': 0,\n",
       "  'type': 'subj_rel'},\n",
       " {'text': 'few senators that the architects like will ever be popular.',\n",
       "  'label': 1,\n",
       "  'type': 'npi_across_anim'},\n",
       " {'text': 'the authors that the chefs love swim.',\n",
       "  'label': 1,\n",
       "  'type': 'obj_rel_across_anim'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
